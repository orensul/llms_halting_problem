<!DOCTYPE html>
<html>
<head>
  <title> LLMs versus the Halting Problem: Revisiting Program Termination Prediction</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>
<style>
  /* ===============================
     HERO LAYOUT
     =============================== */

  .paper-hero {
    padding-top: 2.5rem;
    padding-bottom: 2.5rem;
  }

  /* ===============================
     TITLE
     =============================== */

  .paper-hero .hero-title {
    max-width: 18em;
    margin: 0 auto 0.75rem auto;
    line-height: 1.08;
    letter-spacing: -0.02em;
  }

  /* ===============================
     AUTHORS
     =============================== */

  .paper-hero .publication-authors {
    max-width: 52rem;
    margin-left: auto;
    margin-right: auto;
  }

  .paper-hero .author-block {
    display: inline-block;
    margin: 0.18rem 0.38rem;
    white-space: nowrap;
  }

  .paper-hero .author-block a {
    text-decoration: none;
    border-bottom: 1px solid transparent;
    padding-bottom: 1px;
  }

  .paper-hero .author-block a:hover {
    border-bottom-color: currentColor;
  }

  /* ===============================
     AFFILIATIONS + LOGOS (logo at END)
     =============================== */

  .paper-hero .affiliations {
    font-size: 0.98rem;
    opacity: 0.95;
    margin-top: 0.9rem;
  }

  /* Each affiliation line is centered as a unit */
  .paper-hero .affiliation-row {
    display: flex;
    justify-content: center;
    margin-top: 0.45rem;
  }

  /* Inner content shrinks to fit, keeps "text ... logo" stable */
  .paper-hero .affiliation-row > span,
  .paper-hero .affiliation-row > img {
    display: inline-block;
  }

  .paper-hero .affiliation-row span {
    margin-right: 0.8rem;
  }

  .paper-hero .affiliation-logo {
    height: 28px;
    width: auto;
    max-width: 120px;
    object-fit: contain;
    display: block;
    opacity: 0.92;
    filter: grayscale(100%);
    transition: filter 140ms ease, opacity 140ms ease;
  }

  .paper-hero .affiliation-row:hover .affiliation-logo {
    filter: grayscale(0%);
    opacity: 1;
  }

  /* ===============================
     PUBLICATION LINKS
     =============================== */

  .paper-hero .publication-links {
    margin-top: 1.2rem;
    display: flex;
    justify-content: center;
    flex-wrap: wrap;
    gap: 0.65rem;
  }

  .paper-hero .publication-links .button {
    padding-left: 1.15rem;
    padding-right: 1.15rem;
    box-shadow: 0 10px 26px rgba(0, 0, 0, 0.14);
    transition: transform 140ms ease, box-shadow 140ms ease;
  }

  .paper-hero .publication-links .button:hover {
    transform: translateY(-1px);
    box-shadow: 0 14px 34px rgba(0, 0, 0, 0.18);
  }

  /* ===============================
     HERO TEASER IMAGE (paper artwork)
     =============================== */

  .paper-hero .hero-teaser {
    margin-top: 2.2rem;
    display: flex;
    justify-content: center;
  }

  .paper-hero .hero-teaser img {
    max-width: 85%;
    height: auto;
    border-radius: 12px;
    box-shadow: 0 18px 40px rgba(0, 0, 0, 0.18);
  }

  /* ===============================
     SECTION DIVIDER (between big sections)
     =============================== */

  .section-divider {
    margin: 5rem auto;
    max-width: 60%;
    height: 1px;
    background: linear-gradient(
      to right,
      transparent,
      rgba(0, 0, 0, 0.25),
      transparent
    );
  }

  /* ===============================
     MOBILE TWEAKS
     =============================== */

  @media (max-width: 768px) {
    .paper-hero {
      padding-top: 2rem;
      padding-bottom: 2rem;
    }

    .paper-hero .hero-title {
      max-width: 22em;
    }

    .paper-hero .author-block {
      margin: 0.12rem 0.28rem;
    }

    .paper-hero .affiliation-logo {
      height: 24px;
      max-width: 110px;
    }

    .paper-hero .hero-teaser img {
      max-width: 95%;
    }

    .section-divider {
      max-width: 80%;
      margin: 3.5rem auto;
    }
  }
</style>





<section class="hero paper-hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">

          <h1 class="title is-1 publication-title hero-title">
            LLMs versus the Halting Problem:<br>
            Revisiting Program Termination Prediction
          </h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.orensultan.com/" target="_blank" rel="noopener noreferrer">
                Oren Sultan<sup>1, 2</sup>
              </a>,
            </span>

            <span class="author-block">
              <a href="https://jordiae.com/" target="_blank" rel="noopener noreferrer">
                Jordi Armengol-Estap√©<sup>1</sup>
              </a>,
            </span>

            <span class="author-block">
              <a href="https://www.linkedin.com/in/pascal-kesseli-99561b84/" target="_blank" rel="noopener noreferrer">
                Pascal Kesseli<sup>1</sup>
              </a>,
            </span>

            <span class="author-block">
              <a href="https://www.linkedin.com/in/julienvanegue/" target="_blank" rel="noopener noreferrer">
                Julien Vanegue<sup>3</sup>
              </a>,
            </span>

            <span class="author-block">
              <a href="http://www.hyadatalab.com/" target="_blank" rel="noopener noreferrer">
                Dafna Shahaf<sup>2</sup>
              </a>,
            </span>

            <span class="author-block">
              <a href="https://www.cs.huji.ac.il/~adiyoss/" target="_blank" rel="noopener noreferrer">
                Yossi Adi<sup>1</sup>
              </a>,
            </span>

            <span class="author-block">
              <a href="http://www0.cs.ucl.ac.uk/staff/p.ohearn/" target="_blank" rel="noopener noreferrer">
                Peter O'Hearn<sup>1</sup>
              </a>
            </span>
          </div>

          <div class="is-size-5 publication-authors affiliations">
            <div class="affiliation-row">
              <span>(1) FAIR Team, Meta AI</span>
              <img class="affiliation-logo" src="static/images/meta.jpeg" alt="Meta AI logo">
            </div>

            <div class="affiliation-row">
              <span>(2) The Hebrew University of Jerusalem</span>
              <img class="affiliation-logo" src="static/images/huji.jpeg" alt="Hebrew University logo">
            </div>

            <div class="affiliation-row">
              <span>(3) Bloomberg Research</span>
              <img class="affiliation-logo" src="static/images/bloomberg.jpeg" alt="Bloomberg logo">
            </div>
          </div>

          <div class="publication-links">
            <span class="link-block">
              <a
                href="https://arxiv.org/pdf/2601.18987"
                target="_blank"
                rel="noopener noreferrer"
                class="external-link button is-normal is-rounded is-dark"
              >
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>
          </div>

          <!-- ===== Hero Teaser Image ===== -->
          <div class="hero-teaser">
            <img
              src="static/images/LLMsHaltingProblemImage.jpeg"
              alt="Conceptual illustration of the Halting Problem and Large Language Models"
            >
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<!--               <span class="link-block">
                <a href="https://github.com/orensultan/AIRecolor" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span> -->
      <!--         <span class="link-block">
                <a href="https://arxiv.org/pdf/2410.02952" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->

     <!--          <span class="link-block">
              <a href="https://www.orensultan.com/files/posters/AIRecolorPoster.pdf" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Poster</span>
              </a>
            </span> -->

            <!--   <span class="link-block">
                <a href="https://www.youtube.com/watch?v=wdGOYrtm1Oc" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->


            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>





  <!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Determining whether a program terminates is
a central problem in computer science. Turing‚Äôs foundational result established the Halting
Problem as undecidable, showing that no algorithm can universally determine termination
for all programs and inputs. Consequently,
automatic verification tools approximate termination, sometimes failing to prove or disprove;
these tools rely on problem-specific architectures and abstractions, and are usually tied to particular programming languages. Recent success
and progress in large language models (LLMs)
raises the following question: can LLMs reliably predict program termination? In this
work, we evaluate LLMs on a diverse set of ùê∂
programs from the Termination category of the
International Competition on Software Verification (SV-Comp) 2025. Our results suggest
that LLMs perform remarkably well at predicting program termination, where GPT-5 and
Claude Sonnet-4.5 would rank just behind the
top-ranked tool (using test-time-scaling), and
Code World Model (CWM) would place just
behind the second-ranked tool. While LLMs
are effective at predicting program termination,
they often fail to provide a valid witness as a
proof. Moreover, LLMs performance drops
as program length increases. We hope these
insights motivate further research into program
termination and the broader potential of LLMs
for reasoning about undecidable problems.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


# Section 1 video of the task

  <!-- Teaser video -->
  <section class="hero teaser" style="margin-top: 50px;">
  <div class="container is-max-desktop" style="display: flex; justify-content: center; align-items: center; height: 100vh; flex-direction: column; text-align: center;">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop style="width: 60%; height: 400px;">
        <!-- Your video here -->
        <source src="static/videos/LLMs_vs_Halting_Problem.mp4" type="video/mp4">
      </video>
      <p class="subtitle" style="text-align: left; max-width: 80%; margin: 20px auto 0;">
  <b>Termination prediction task.</b><br>
  Given a program, the task is to predict whether the program halts for all inputs
  (instantiations of nondeterministic variables).<br>
  <b>(1)</b>
  Traditional verification tools such as <span style="font-variant: small-caps;">PROTON</span>
  rely on multi-component architectures for parsing, input augmentation, and tool-chain management
  <b>(2)</b>
  We investigate whether large language models (LLMs) can match state-of-the-art verification tools,
  offering a simpler, language-agnostic solution, as illustrated in the example above where the LLM
  correctly predicts termination.
</p>

    </div>
  </div>
</section>

  <!-- End teaser video -->


<div class="section-divider"></div>

# Section 2 LLM witness prediction

<!-- Image Section for "Our Task" -->
<section class="image-section" style="margin-top: 100px;">
  <div class="container is-max-desktop" style="display: flex; flex-direction: column; justify-content: center; align-items: center; text-align: center; padding: 20px;">
    <img src="static/images/witness_example.jpeg" alt="Website" style="max-width: 80%; height: auto; margin-bottom: 30px;"> <!-- Increased margin-bottom -->
<p class="subtitle" style="text-align: left; max-width: 80%; margin: 20px auto 0;">
    <b>LLM non-termination witness prediction.</b><br>
  Given a <i>C</i> program, an LLM that predicts <i>nontermination</i> must additionally output
  a <b>witness automaton</b> as a proof in JSON format (see Figure&nbsp;8 in the Appendix).
  The witness automaton models a potentially infinite execution:
  nodes correspond to program states, and edges correspond to transitions.<br>

  The predicted JSON is converted to <span style="font-variant: small-caps;">GraphML</span>
  and validated using a witness validator (e.g.,
  <span style="font-variant: small-caps;">UAutomizer</span>).
  The example illustrates a loop in which the variable
  <i>i</i> is initialized in the range
  [‚àí5, 5], eventually reaches 0 (as specified by the assumption on edge E2),
  and then executes indefinitely.
</p>

  </div>
</section>

<!-- Image Section for "Our Distillation Framework" -->
<section class="image-section" style="margin-top: 100px;"> <!-- Adjusted margin-top -->
  <div class="container is-max-desktop" style="display: flex; flex-direction: column; justify-content: center; align-items: center; text-align: center; padding: 20px;">
    <h2 class="title is-3">SV-Comp main results</h2>
    <img src="static/images/main_results.jpeg" alt="Website" style="max-width: 80%; height: auto; margin-bottom: 30px;"> <!-- Increased margin-bottom -->
    <p class="subtitle" style="text-align: left; max-width: 80%; margin: 20px auto 0;">
  Mean <span style="font-variant: small-caps;">SV-Comp</span> scores for LLMs
  (averaged over 100 bootstrap samples), the top
  <span style="font-variant: small-caps;">SV-Comp&nbsp;2025</span> verification tools,
  and the maximum achievable score
  (minimum possible score: ‚àí50,064).<br><br>

  <span style="font-variant: small-caps;">GPT-5</span> with test-time scaling (TTS)
  and <span style="font-variant: small-caps;">Claude&nbsp;Sonnet-4.5</span> (TTS)
  would rank 2nd and 3rd with scores of 3,520 and 3,448, respectively,
  trailing the gold-medal system
  <span style="font-variant: small-caps;">PROTON</span>.<br><br>

  <span style="font-variant: small-caps;">CWM</span> ranks just below
  <span style="font-variant: small-caps;">UAutomizer</span>.
  In contrast, <span style="font-variant: small-caps;">GPT-4o</span>
  performs substantially worse, scoring 546 with TTS and ‚àí5,145 without TTS.
</p>

  </div>
</section>





<!-- Image carousel -->
<!-- <section class="hero is-small" style="margin-top: 150px;">
  <div class="hero-body" style="display: flex; justify-content: center; align-items: center;">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel" style="display: flex; justify-content: center; align-items: center;">
        <div class="item" style="text-align: center;">
          <h2 class="title is-3">Experiments</h2>
          <img src="static/images/exp1.jpg" style="max-width: 80%; height: auto; margin-bottom: 15px;">
          <p class="subtitle" style="text-align: left; max-width: 80%; margin: 20px auto 0;">
            Offline evaluation results for our student models. Metrics include (tool-selection score, quality score, final score), and the average final score across the tools (Overall).
            Results show that FlanT5-base performs very similarly to Llama-2-7b-chat-hf, with only a 0.02 gap (rows 1, 4).
            Interestingly, both models perform better on a test subset with more popular user intents (r_5 > r_3 > All), where r_i denotes user intents with at least i calls.
          </p>
        </div>
        <div class="item" style="text-align: center;">
          <img src="static/images/reality_check.jpg" style="max-width: 80%; height: auto; margin-bottom: 15px;">
          <p class="subtitle" style="text-align: left; max-width: 80%; margin: 20px auto 0;">
            Output images for reality check. Here are examples of samples given to our annotators to evaluate.
            For each sample, they were asked two binary questions:
            (1) whether the image is relevant to the intent, and (2) whether the student models correctly mimic the teacher model (see Section 4.2).
            Each sample includes the source image and the outputs of the teacher LLM along with the outputs from both of our student LLMs.
            Based on the annotator‚Äôs majority vote: In the first sample: (1) All models produced results relevant to the intent ‚ÄúMorocco‚Äù (e.g., warm hues, typical of Moroccan landscapes, reflecting its deserts).
            (2) Both student models successfully mimicked the teacher LLM.
            In the second sample: (1) All models produced results relevant to the intent ‚ÄúThe Matrix‚Äù (e.g., darkness, green tint, and cyberpunk aesthetic)
            (2) Both student models did not mimic the teacher LLM well.
          </p>
        </div>
        <div class="item" style="text-align: center;">
          <img src="static/images/online_ab_test_exps.jpg" style="max-width: 80%; height: auto; margin-bottom: 15px;">
          <p class="subtitle" style="text-align: left; max-width: 80%; margin: 20px auto 0;">
             In addition to offline evaluation, we conducted two online A/B tests. <br>
             First, we compared our teacher, GPT-3.5-Turbo (tested on 94,317 projects), with Llama-2-7b-chat-hf (93,495 projects).
             We measured project completion rates as an indicator of user satisfaction.
             The completion rate for the teacher was 96.1% of that of Llama-2-7b-chat-hf (no statistical significance). Thus, we conclude they are comparable.  <br>
             In our second A/B test, we compared our student models. FlanT5-base (tested on 20,294 projects) achieved a completion rate of 99% of that of Llama-2-7b-chat-hf (20,282 projects).
             Thus, we conclude they are comparable and choose FlanT5-base for its lower latency and cost. <br>
             Importantly, we are encouraged by the fact that our offline metrics align with the results of the online A/B tests
          </p>
        </div>

        <div class="item" style="text-align: center;">
          <img src="static/images/exp2.jpg" style="max-width: 80%; height: auto; margin-bottom: 15px;">
          <p class="subtitle" style="text-align: left; max-width: 80%; margin: 20px auto 0;">
            FlanT5-base‚Äôs performance in subsets of the train set, with and without augmentation.
            We can see that augmentation is effective in limited data increasing the overall score by 0.13 for the 1/8 sample.
            With larger training subsets, the proportion of augmentations (%) decreases, reducing overall improvement as expected.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End image carousel -->


<!--  &lt;!&ndash; Youtube video &ndash;&gt;-->
<!--  <section class="hero is-small is-light" style="margin-top: 100px;">-->
<!--    <div class="hero-body">-->
<!--      <div class="container">-->
<!--        <h2 class="title is-3">Lightricks Real Use-case: Visual Editing with LLM-based Tool Chaining</h2>-->
<!--        <div class="columns is-centered has-text-centered">-->
<!--          <div class="column is-four-fifths">-->
<!--            <div class="publication-video">-->
<!--              <iframe src="https://www.youtube.com/embed/XWM7MM4M2Ws" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
<!--            </div>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </section>-->
<!--  &lt;!&ndash; End youtube video &ndash;&gt;-->

 <!--  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">EMNLP 2024 Poster</h2>
        <iframe src="static/pdfs/AIRecolorPoster.pdf" width="100%" height="550"></iframe>
      </div>
    </div>
  </section> -->

  <!-- BibTex citation -->
  <section class="section" id="BibTeX" style="margin-top: 100px;">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{sultan2024visualediting,
  title={Visual Editing with LLM-based Tool Chaining: An Efficient Distillation Approach for Real-Time Applications},
  author={Sultan, Oren and Khasin, Alex and Shiran, Guy and Greenstein-Messica, Asnat and Shahaf, Dafna},
  journal={arXiv preprint arXiv:2210.12197},
  year={2024}
}</code></pre>
  </div>
</section>

  <!-- End BibTeX citation -->

<!--  <footer class="footer">-->
<!--    <div class="container">-->
<!--      <div class="columns is-centered">-->
<!--        <div class="column is-8">-->
<!--          <div class="content">-->
<!--            <p>-->
<!--              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.-->
<!--              <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative-->
<!--              Commons Attribution-ShareAlike 4.0 International License</a>.-->
<!--            </p>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </footer>-->

  <!-- Statcounter tracking code -->
  <script type="text/javascript">
  var sc_project=12822541;
  var sc_invisible=1;
  var sc_security="46936e0e";
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript><div class="statcounter"><a title="Web Analytics"
  href="https://statcounter.com/" target="_blank"><img class="statcounter"
  src="https://c.statcounter.com/12822541/0/46936e0e/1/" alt="Web Analytics"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->

</body>
</html>
